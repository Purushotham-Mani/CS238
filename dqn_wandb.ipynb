{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import highway_env\n",
    "\n",
    "import wandb\n",
    "import json\n",
    "import warnings\n",
    "import random\n",
    "import time\n",
    "# Suppress DeprecationWarning\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class DQNAgent:\n",
    "    def __init__(self, agent_config_dict):\n",
    "        # Extract all configurations from the dictionary\n",
    "        self.env = agent_config_dict[\"env\"]\n",
    "        self.model = agent_config_dict[\"model\"]\n",
    "        self.gamma = agent_config_dict.get(\"gamma\", 0.99)  # Discount Factor\n",
    "        self.epsilon = agent_config_dict.get(\"epsilon\", 1.0)  # epsilon for epsilon-greedy\n",
    "        self.epsilon_decay = agent_config_dict.get(\"epsilon_decay\", 0.995)\n",
    "        self.epsilon_min = agent_config_dict.get(\"epsilon_min\", 0.1)\n",
    "        self.batch_size = agent_config_dict.get(\"batch_size\", 64)\n",
    "        self.replay_capacity = agent_config_dict.get(\"replay_capacity\", 10000)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=agent_config_dict.get(\"lr\", 1e-3))\n",
    "\n",
    "        self.render_scene = agent_config_dict.get(\"render_scene\", False)\n",
    "        self.logs = {}\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.replay_buffer = []\n",
    "        self.replay_buffer_scenes = []\n",
    "\n",
    "        self.wandb_log = agent_config_dict[\"wandb_log\"]\n",
    "        # Initialize wandb\n",
    "        if self.wandb_log:\n",
    "            wandb.init(project=agent_config_dict.get(\"wandb_project\", \"dqn_highway_gym\"))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                return torch.argmax(self.model(state)).item()\n",
    "\n",
    "    def store_transition(self, episode, step, state, action, reward, next_state, done,scene_arr,log):\n",
    "        if len(self.replay_buffer) >= self.replay_capacity:\n",
    "            self.replay_buffer.pop(0)\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "        if log == True:\n",
    "            if episode in self.logs:\n",
    "                self.logs[episode][\"step\"].append(step)\n",
    "                self.logs[episode][\"state\"].append(state)\n",
    "                self.logs[episode][\"action\"].append(action)\n",
    "                self.logs[episode][\"reward\"].append(reward)\n",
    "                self.logs[episode][\"next_state\"].append(next_state)\n",
    "                self.logs[episode][\"done\"].append(done)\n",
    "                self.logs[episode][\"scene_arr\"].append(scene_arr)\n",
    "\n",
    "            else:\n",
    "                self.logs[episode] = {\"step\":[],\n",
    "                                    \"state\":[],\n",
    "                                    \"action\":[],\n",
    "                                    \"reward\":[],\n",
    "                                    \"next_state\":[],\n",
    "                                    \"done\":[],\n",
    "                                    \"scene_arr\":[]}\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return (None,None)\n",
    "\n",
    "        batch = random.sample(self.replay_buffer, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "        # Compute Q-values\n",
    "        # print(states.shape)\n",
    "        # print(actions.shape)\n",
    "        q_values = self.model(states).gather(1, actions).squeeze(1)\n",
    "\n",
    "        q_val_norm = 0\n",
    "        # print(\"passed\")\n",
    "        # Compute target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_states).max(1)[0]\n",
    "            q_val_norm = (np.linalg.norm(next_q_values))\n",
    "            targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "\n",
    "        # Loss\n",
    "        loss = self.criterion(q_values, targets)\n",
    "\n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "        return (q_val_norm,-1)\n",
    "\n",
    "    def play_episode(self, episode_num,train=True,log=False):\n",
    "        state, info = self.env.reset()\n",
    "        state_flattened = state.reshape(-1)\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "        q_val_norm = None\n",
    "        loss = None\n",
    "\n",
    "        while True:\n",
    "            action = self.select_action(state_flattened)\n",
    "            next_state, reward, done, truncated, _ = self.env.step(action)\n",
    "\n",
    "            if self.render_scene:\n",
    "                scene_arr = self.env.render()\n",
    "            else:\n",
    "                scene_arr = None\n",
    "                \n",
    "            state_flattened = state.reshape(-1)\n",
    "            next_state_flattened = next_state.reshape(-1)\n",
    "            self.store_transition(episode_num,step, state_flattened, action, reward, next_state_flattened, done or truncated,scene_arr,log)\n",
    "            if train:\n",
    "                q_val_norm,loss = self.train()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "\n",
    "            if done or truncated:\n",
    "                break\n",
    "\n",
    "        # Logging\n",
    "\n",
    "\n",
    "        if train:\n",
    "            episode_log = {\n",
    "                \"train/total_reward\": total_reward,\n",
    "                \"train/last_step_reward\": reward,\n",
    "                \"train/episode_length\": step,\n",
    "                \"train/epsilon\":self.epsilon,\n",
    "                \"train/Q_val_norm\":q_val_norm,\n",
    "                \"train/loss\":loss\n",
    "            }\n",
    "        else:\n",
    "            episode_log = {\n",
    "                \"test/total_reward\": total_reward,\n",
    "                \"test/last_step_reward\": reward,\n",
    "                \"test/episode_length\": step\n",
    "            }\n",
    "\n",
    "\n",
    "        print(\"Episode: {:<3} | Episode Length: {:<3} | Epsilon: {:<3.3f} | Total Reward: {:<4.3f} | Last Step Reward: {} | loss: {} | Q val norm: {}\".format(episode_num, step, self.epsilon,total_reward, reward,loss,q_val_norm))\n",
    "        if self.wandb_log:\n",
    "            wandb.log(episode_log)\n",
    "\n",
    "        return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.19.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/chetan/Desktop/Fall 2024/CS238 Decision Making under Uncertainty/Projects/Final Project/wandb/run-20241207_174133-bzc61b1p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/crna/RoadSense%20Project/runs/bzc61b1p' target=\"_blank\">ruby-night-9</a></strong> to <a href='https://wandb.ai/crna/RoadSense%20Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/crna/RoadSense%20Project' target=\"_blank\">https://wandb.ai/crna/RoadSense%20Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/crna/RoadSense%20Project/runs/bzc61b1p' target=\"_blank\">https://wandb.ai/crna/RoadSense%20Project/runs/bzc61b1p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0   | Episode Length: 40  | Epsilon: 1.000 | Total Reward: 28.656 | Last Step Reward: 0.7951727812171602 | loss: None | Q val norm: None\n",
      "Episode: 1   | Episode Length: 8   | Epsilon: 1.000 | Total Reward: 6.390 | Last Step Reward: 0.06666666666666665 | loss: None | Q val norm: None\n",
      "Episode: 2   | Episode Length: 6   | Epsilon: 1.000 | Total Reward: 4.515 | Last Step Reward: 0.09496034658703383 | loss: None | Q val norm: None\n",
      "Episode: 3   | Episode Length: 4   | Epsilon: 1.000 | Total Reward: 2.757 | Last Step Reward: 0.09440737566895548 | loss: None | Q val norm: None\n",
      "Episode: 4   | Episode Length: 12  | Epsilon: 0.932 | Total Reward: 8.617 | Last Step Reward: 0.04444444444444443 | loss: -1 | Q val norm: 2.7980899810791016\n",
      "Episode: 5   | Episode Length: 26  | Epsilon: 0.718 | Total Reward: 19.016 | Last Step Reward: 0.0 | loss: -1 | Q val norm: 32.89784240722656\n",
      "Episode: 6   | Episode Length: 40  | Epsilon: 0.480 | Total Reward: 30.337 | Last Step Reward: 0.7771992679041103 | loss: -1 | Q val norm: 252.7894287109375\n",
      "Episode: 7   | Episode Length: 40  | Epsilon: 0.321 | Total Reward: 30.222 | Last Step Reward: 0.6888893731035366 | loss: -1 | Q val norm: 540.3494873046875\n",
      "Episode: 8   | Episode Length: 18  | Epsilon: 0.268 | Total Reward: 12.378 | Last Step Reward: 0.04444444444444443 | loss: -1 | Q val norm: 537.9693603515625\n",
      "Episode: 9   | Episode Length: 40  | Epsilon: 0.179 | Total Reward: 30.382 | Last Step Reward: 0.7333333470725282 | loss: -1 | Q val norm: 376.5147705078125\n",
      "Episode: 10  | Episode Length: 40  | Epsilon: 0.120 | Total Reward: 28.339 | Last Step Reward: 0.7111111111111116 | loss: -1 | Q val norm: 307.1907653808594\n",
      "Episode: 11  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 30.072 | Last Step Reward: 0.7111111111111116 | loss: -1 | Q val norm: 367.9947204589844\n",
      "Episode: 12  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.515 | Last Step Reward: 0.7111111111111111 | loss: -1 | Q val norm: 288.8271484375\n",
      "Episode: 13  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 28.183 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 279.1811218261719\n",
      "Episode: 14  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 28.161 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 286.453857421875\n",
      "Episode: 15  | Episode Length: 25  | Epsilon: 0.100 | Total Reward: 17.886 | Last Step Reward: 0.04444444444444443 | loss: -1 | Q val norm: 298.62322998046875\n",
      "Episode: 16  | Episode Length: 4   | Epsilon: 0.100 | Total Reward: 2.599 | Last Step Reward: 0.07080510083096499 | loss: -1 | Q val norm: 297.31207275390625\n",
      "Episode: 17  | Episode Length: 2   | Epsilon: 0.100 | Total Reward: 1.041 | Last Step Reward: 0.19823985164738409 | loss: -1 | Q val norm: 293.7542419433594\n",
      "Episode: 18  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.226 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 265.6004638671875\n",
      "Episode: 19  | Episode Length: 15  | Epsilon: 0.100 | Total Reward: 11.932 | Last Step Reward: 0.06666666666666665 | loss: -1 | Q val norm: 272.6585388183594\n",
      "Episode: 20  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.827 | Last Step Reward: 0.7116627924635169 | loss: -1 | Q val norm: 240.90663146972656\n",
      "Episode: 21  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 28.627 | Last Step Reward: 0.7333333333333334 | loss: -1 | Q val norm: 277.8144226074219\n",
      "Episode: 22  | Episode Length: 10  | Epsilon: 0.100 | Total Reward: 7.952 | Last Step Reward: 0.06666666666666665 | loss: -1 | Q val norm: 275.0301513671875\n",
      "Episode: 23  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.139 | Last Step Reward: 0.6888888888888892 | loss: -1 | Q val norm: 268.5740661621094\n",
      "Episode: 24  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.027 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 287.1363830566406\n",
      "Episode: 25  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 28.761 | Last Step Reward: 0.7111111111111111 | loss: -1 | Q val norm: 308.665771484375\n",
      "Episode: 26  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.583 | Last Step Reward: 0.6888888888888892 | loss: -1 | Q val norm: 305.28790283203125\n",
      "Episode: 27  | Episode Length: 5   | Epsilon: 0.100 | Total Reward: 3.200 | Last Step Reward: 0.0 | loss: -1 | Q val norm: 280.6471862792969\n",
      "Episode: 28  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.090 | Last Step Reward: 0.6666666666666671 | loss: -1 | Q val norm: 255.2881622314453\n",
      "Episode: 29  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 28.916 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 269.9095764160156\n",
      "Episode: 30  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 28.159 | Last Step Reward: 0.6666666666666671 | loss: -1 | Q val norm: 212.33824157714844\n",
      "Episode: 31  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 28.050 | Last Step Reward: 0.6666666666666666 | loss: -1 | Q val norm: 208.7799530029297\n",
      "Episode: 32  | Episode Length: 3   | Epsilon: 0.100 | Total Reward: 1.622 | Last Step Reward: 0.022222222222222216 | loss: -1 | Q val norm: 215.29469299316406\n",
      "Episode: 33  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.694 | Last Step Reward: 0.6666666666666671 | loss: -1 | Q val norm: 240.2281494140625\n",
      "Episode: 34  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.271 | Last Step Reward: 0.7333333333333334 | loss: -1 | Q val norm: 236.04600524902344\n",
      "Episode: 35  | Episode Length: 3   | Epsilon: 0.100 | Total Reward: 1.619 | Last Step Reward: 0.0 | loss: -1 | Q val norm: 230.85641479492188\n",
      "Episode: 36  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.361 | Last Step Reward: 0.688888888888889 | loss: -1 | Q val norm: 240.1725311279297\n",
      "Episode: 37  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.224 | Last Step Reward: 0.6666827773006491 | loss: -1 | Q val norm: 224.3651580810547\n",
      "Episode: 38  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 26.959 | Last Step Reward: 0.6666666666666671 | loss: -1 | Q val norm: 231.71116638183594\n",
      "Episode: 39  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.401 | Last Step Reward: 0.688888888888889 | loss: -1 | Q val norm: 237.95831298828125\n",
      "Episode: 40  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.361 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 270.1823425292969\n",
      "Episode: 41  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.250 | Last Step Reward: 0.6666666667352309 | loss: -1 | Q val norm: 346.7298889160156\n",
      "Episode: 42  | Episode Length: 20  | Epsilon: 0.100 | Total Reward: 13.583 | Last Step Reward: 0.04444444444444443 | loss: -1 | Q val norm: 320.2777099609375\n",
      "Episode: 43  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.602 | Last Step Reward: 0.7111111111111116 | loss: -1 | Q val norm: 300.25634765625\n",
      "Episode: 44  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.916 | Last Step Reward: 0.688888888888889 | loss: -1 | Q val norm: 250.08566284179688\n",
      "Episode: 45  | Episode Length: 8   | Epsilon: 0.100 | Total Reward: 6.853 | Last Step Reward: 0.27997959724323285 | loss: -1 | Q val norm: 249.579833984375\n",
      "Episode: 46  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.850 | Last Step Reward: 0.6888888888888892 | loss: -1 | Q val norm: 258.2831726074219\n",
      "Episode: 47  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.072 | Last Step Reward: 0.6666666666666671 | loss: -1 | Q val norm: 272.3104248046875\n",
      "Episode: 48  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.361 | Last Step Reward: 0.6666666666666671 | loss: -1 | Q val norm: 283.9100036621094\n",
      "Episode: 49  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.494 | Last Step Reward: 0.6666666667352309 | loss: -1 | Q val norm: 220.64285278320312\n",
      "Episode: 50  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 30.380 | Last Step Reward: 0.7111111111114533 | loss: -1 | Q val norm: 298.9521789550781\n",
      "Episode: 51  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.823 | Last Step Reward: 0.8627729614955256 | loss: -1 | Q val norm: 260.8127746582031\n",
      "Episode: 52  | Episode Length: 4   | Epsilon: 0.100 | Total Reward: 2.624 | Last Step Reward: 0.09185185139019743 | loss: -1 | Q val norm: 254.15504455566406\n",
      "Episode: 53  | Episode Length: 4   | Epsilon: 0.100 | Total Reward: 2.400 | Last Step Reward: 0.0 | loss: -1 | Q val norm: 247.68650817871094\n",
      "Episode: 54  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 26.805 | Last Step Reward: 0.6666666666666671 | loss: -1 | Q val norm: 207.35494995117188\n",
      "Episode: 55  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 26.827 | Last Step Reward: 0.6698949834294382 | loss: -1 | Q val norm: 236.9057159423828\n",
      "Episode: 56  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.139 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 229.0771942138672\n",
      "Episode: 57  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.294 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 218.72958374023438\n",
      "Episode: 58  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.337 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 226.79795837402344\n",
      "Episode: 59  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.271 | Last Step Reward: 0.7333333333333334 | loss: -1 | Q val norm: 281.2641906738281\n",
      "Episode: 60  | Episode Length: 9   | Epsilon: 0.100 | Total Reward: 7.662 | Last Step Reward: 0.06666666666666665 | loss: -1 | Q val norm: 297.7018737792969\n",
      "Episode: 61  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.227 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 301.3801574707031\n",
      "Episode: 62  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 36.003 | Last Step Reward: 0.7333366538962719 | loss: -1 | Q val norm: 328.4919738769531\n",
      "Episode: 63  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 26.761 | Last Step Reward: 0.6666666666666671 | loss: -1 | Q val norm: 258.4421691894531\n",
      "Episode: 64  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.737 | Last Step Reward: 0.7333333333333334 | loss: -1 | Q val norm: 282.9854736328125\n",
      "Episode: 65  | Episode Length: 9   | Epsilon: 0.100 | Total Reward: 6.978 | Last Step Reward: 0.04444444444444443 | loss: -1 | Q val norm: 277.63641357421875\n",
      "Episode: 66  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.583 | Last Step Reward: 0.6888888888888892 | loss: -1 | Q val norm: 284.2716369628906\n",
      "Episode: 67  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 27.361 | Last Step Reward: 0.6666666666666671 | loss: -1 | Q val norm: 230.17454528808594\n",
      "Episode: 68  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 26.939 | Last Step Reward: 0.6667609425190552 | loss: -1 | Q val norm: 225.3393096923828\n",
      "Episode: 69  | Episode Length: 40  | Epsilon: 0.100 | Total Reward: 29.248 | Last Step Reward: 0.7333333333333337 | loss: -1 | Q val norm: 236.91217041015625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load configuration from file\n",
    "with open('env_config_dict.txt', 'r') as f:\n",
    "    env_config_dict = json.load(f)\n",
    "\n",
    "\n",
    "# env = gym.make(\"intersection-v0\", render_mode=\"rgb_array\", config=env_config_dict)\n",
    "env = gym.make(\"highway-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "input_dim = env.observation_space.shape[0]*env.observation_space.shape[1] # Flattening the matrix\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "# Create the model\n",
    "model = DQNModel(input_dim, output_dim)\n",
    "\n",
    "agent_config_dict = {\n",
    "    \"env\": env,\n",
    "    \"model\": model,\n",
    "    \"gamma\": 0.99,\n",
    "    \"epsilon\": 1.0,\n",
    "    \"epsilon_decay\": 0.99,\n",
    "    \"epsilon_min\": 0.1,\n",
    "    \"batch_size\": 64,\n",
    "    \"replay_capacity\": 10000,\n",
    "    \"lr\": 1e-3,\n",
    "    \"wandb_log\": True,\n",
    "    \"wandb_project\": \"RoadSense Project\",\n",
    "    \"render_scene\":False\n",
    "}\n",
    "\n",
    "agent = DQNAgent(agent_config_dict)\n",
    "\n",
    "# # Training loop\n",
    "num_train_episodes = 100\n",
    "for episode in range(num_train_episodes):\n",
    "    reward = agent.play_episode(episode_num = episode, train=True,log=False)\n",
    "\n",
    "print()\n",
    "print(\"Evaluation\")\n",
    "\n",
    "# Evaluation loop\n",
    "num_eval_episodes = 10\n",
    "for episode in range(num_eval_episodes):\n",
    "    reward = agent.play_episode(episode_num = episode, train=False,log=False)\n",
    "    # print(f\"Evaluation Episode {episode + 1}: Total Reward: {reward}\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "def run_eps(env, num_eps):\n",
    "\n",
    "    ep = 0\n",
    "    step = 0\n",
    "    observation, info = env.reset()\n",
    "    rewards_eps = {}\n",
    "    obs_eps = {}\n",
    "    actions_eps = {}\n",
    "    rendered_eps = {}\n",
    "    length_eps = {}\n",
    "\n",
    "    # Run for a fixed number of steps\n",
    "    while ep<num_eps:\n",
    "        reward_ep_list = []\n",
    "        obs_ep_list = []\n",
    "        action_ep_list = []\n",
    "        rendered_ep_list = []\n",
    "\n",
    "        while True:\n",
    "            # Choose a random action\n",
    "            # action =  env.action_space.sample()\n",
    "            action = agent.get_action(observation.reshape(-1))\n",
    "            \n",
    "            # Take a step in the environment\n",
    "            present_time = time.time()\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            after_time = time.time()\n",
    "\n",
    "            step_time_duration = after_time - present_time\n",
    "\n",
    "\n",
    "            # Render the environment\n",
    "            scene_arr = env.render()\n",
    "\n",
    "            reward_ep_list.append(reward)\n",
    "            obs_ep_list.append(observation)\n",
    "            action_ep_list.append(action)\n",
    "            rendered_ep_list.append(scene_arr)\n",
    "\n",
    "            # Check if the episode is done\n",
    "            if terminated or truncated:\n",
    "                observation, info = env.reset()\n",
    "                rewards_eps[ep] = reward_ep_list\n",
    "                obs_eps[ep] = obs_ep_list\n",
    "                actions_eps[ep] = action_ep_list\n",
    "                rendered_eps[ep] = rendered_ep_list\n",
    "                length_eps[ep] = step\n",
    "\n",
    "                print(\"Episode: {} | Num Time Steps: {} | Terminated\".format(ep,step))\n",
    "                step = 0\n",
    "                ep += 1\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "            step +=1 \n",
    "        # time.sleep(0.1)  # Delay for 0.1 seconds\n",
    "\n",
    "    info = {\"rewards_eps\":rewards_eps,\n",
    "            \"obs_eps\":obs_eps,\n",
    "            \"actions_eps\":actions_eps,\n",
    "            \"rendered_eps\":rendered_eps,\n",
    "            \"length_eps\":length_eps}\n",
    "    # Close the environment\n",
    "    env.close()\n",
    "    return info\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
